
kafka ---distributed messaging queue,also acts as an input buffer to queue th input stream 

#Create Topic
bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic word-count


# Create DataSet representing the stream of input lines from kafka
lines = spark\
    .readStream\
    .format("kafka")\
    .option("kafka.bootstrap.servers", "localhost:9092")\
    .option("subscribe", "word-count")\
    .load()\
    .selectExpr("CAST(value AS STRING)")

     # Split the lines into words
words = lines.select(
    # explode turns each item in an array into a separate row
    explode(
        split(lines.value, ' ')
    ).alias('word')
)

 wordCounts = words.groupBy('word').count()

# Start running the query that prints the running counts to the console
query = wordCounts\
    .writeStream\
    .outputMode('complete')\
    .format('console')\
    .start()


bin/kafka-console-producer.sh --broker-list localhost:9092 --topic word-count

----------------------------------------------------------------------------------
        #Navigate to KAFKA HOME directory
        cd $KAFKA_HOME

        ##Kafka commands
        #Create Topic
        bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test

        #List Topics
        bin/kafka-topics.sh --list --bootstrap-server localhost:9092

        #Describe Topic
        bin/kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic test

        #Produce Message
        bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test

        #Consume Message (from current subscription)
        bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test
        #Consume Message (from beginning)
        bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning
        #Consume Message (with consumer group)
        bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --group test-group
        #Consume Message (from specific partition)
        bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --partition 0 --offset earliest
        #Consume Message (from specific offset)
        bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --partition 0 --offset 3

        #Alter Topic
        bin/kafka-topics.sh --alter --bootstrap-server localhost:9092 --topic test --partitions 3

        #Delete Topic
        bin/kafka-topics.sh --delete --bootstrap-server localhost:9092 --topic test

        #Consumer Groups
        bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list
        bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group test-group
        bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --all-groups





-----------------------------------------------------------------------------------
pyspark --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1

from pyspark.sql.types import TimestampType, StringType, StructField, StructType
from pyspark.sql.functions import *

bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic loans
bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic loans-processed

#Consume message from Kafka topic and create Dataset
loansStreamingDF = spark.readStream.format("kafka")\
	                .option("kafka.bootstrap.servers", "localhost:9092")\
                    .option("subscribe", "loans")\
                    .option("kafka.group.id", "loans-consumer")\
	                .option("startingOffsets", "earliest")\
	                .load()

loansStreamingDF.printSchema()


#Schema definition for consuming message
schema = StructType([ StructField("time", TimestampType(), True),
                      StructField("customer", StringType(), True),
                      StructField("loanId", StringType(), True),
                      StructField("status", StringType(), True)])

#Converting received kafka binary message to json message applying custom schema
loansStreamingJsonDF = loansStreamingDF.select(from_json(col("value").cast("String"), schema).alias("loans"))


#Print Schema
loansStreamingJsonDF.printSchema()

loansFlattenedDF = loansStreamingJsonDF.select("loans.time","loans.customer","loans.loanId","loans.status")

#Print Schema
loansFlattenedDF.printSchema()

#Group the data by window and status and compute the count of each group
loanStatusCountsWindowedDF = loansFlattenedDF\
    .groupBy( \
    window(loansFlattenedDF.time, "60 seconds", "30 seconds"),
    loansFlattenedDF.status
).count().orderBy('window')

*****
loanStatusCountsWindowedDF = loansFlattenedDF\
    .groupBy( \
   loansFlattenedDF.status
).count().orderBy('window')   *****


#Print Schema
loanStatusCountsWindowedDF.printSchema()

outputDF =  loanStatusCountsWindowedDF\
                .selectExpr("CAST(status AS STRING) AS key", "to_json(struct(*)) AS value")



query = outputDF\
    .writeStream \
    .outputMode("complete") \
    .format("kafka") \
    .option("kafka.bootstrap.servers","localhost:9092") \
    .option("topic","loans-processed") \
    .option("checkpointLocation", "loans") \
    .start()

bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic loans-processed

--------------------------------------------
query = outputDF\
    .writeStream \
    .outputMode("complete") \
    .format("console") \
    .option("checkpointLocation", "loans") \
    .start()

-----------------------------


