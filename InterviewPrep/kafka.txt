
kafka ---distributed messaging queue,also acts as an input buffer to queue th input stream 

message semantics:
    --Queue ---FIFO---point to point communication
    --topic --pub sub --one to many


kafka Architecture
    --producer   --produce the events
    --broker     --stores and process the events
    --consumer   --consumes the events
    --zookeper -- co-ordibation service and  maintains metadata 

producer-->broker--->partitions
kafka concepts:
    message/event
        -topic
            -partitions (deafualt 1 partition , 1 replication)
                (in replicas there will be leader partition and folower/in sync replica(ISR))
                zookeper will elect any ISR as leader ,when leader is down
        --offset : it will be specific to each partition to uniquiley identify message in the partition

        ---order of messages is not guarenteed if there multiple partitions for a topic

        --consumer group : with multiple consumer instances ,we can consume the messages parallely with faster rate
           (ideally no consumer instances should be equal to number of partitions)

---while connecting to kafka server....we can give any broker ip and port of that server




#Create Topic
bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic word-count


# Create DataSet representing the stream of input lines from kafka
lines = spark\
    .readStream\
    .format("kafka")\
    .option("kafka.bootstrap.servers", "localhost:9092")\
    .option("subscribe", "word-count")\
    .load()\
    .selectExpr("CAST(value AS STRING)")

     # Split the lines into words
words = lines.select(
    # explode turns each item in an array into a separate row
    explode(
        split(lines.value, ' ')
    ).alias('word')
)

 wordCounts = words.groupBy('word').count()

# Start running the query that prints the running counts to the console
query = wordCounts\
    .writeStream\
    .outputMode('complete')\
    .format('console')\
    .start()


bin/kafka-console-producer.sh --broker-list localhost:9092 --topic word-count

----------------------------------------------------------------------------------
        #Navigate to KAFKA HOME directory
        cd $KAFKA_HOME

        ##Kafka commands
        #Create Topic
        bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test

        #List Topics
        bin/kafka-topics.sh --list --bootstrap-server localhost:9092

        #Describe Topic
        bin/kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic test

        #Produce Message
        bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test

        #Consume Message (from current subscription)
        bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test
        #Consume Message (from beginning)
        bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning
        #Consume Message (with consumer group)
        bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --group test-group
        #Consume Message (from specific partition)
        bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --partition 0 --offset earliest
        #Consume Message (from specific offset)
        bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --partition 0 --offset 3

        #Alter Topic
        bin/kafka-topics.sh --alter --bootstrap-server localhost:9092 --topic test --partitions 3

        #Delete Topic
        bin/kafka-topics.sh --delete --bootstrap-server localhost:9092 --topic test

        #Consumer Groups
        bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list
        bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group test-group
        bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --all-groups





-----------------------------------------------------------------------------------
pyspark --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1

from pyspark.sql.types import TimestampType, StringType, StructField, StructType
from pyspark.sql.functions import *

bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic loans
bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic loans-processed

#Consume message from Kafka topic and create Dataset
loansStreamingDF = spark.readStream.format("kafka")\
	                .option("kafka.bootstrap.servers", "localhost:9092")\
                    .option("subscribe", "loans")\
                    .option("kafka.group.id", "loans-consumer")\
	                .option("startingOffsets", "earliest")\
	                .load()

loansStreamingDF.printSchema()


#Schema definition for consuming message
schema = StructType([ StructField("time", TimestampType(), True),
                      StructField("customer", StringType(), True),
                      StructField("loanId", StringType(), True),
                      StructField("status", StringType(), True)])

#Converting received kafka binary message to json message applying custom schema
loansStreamingJsonDF = loansStreamingDF.select(from_json(col("value").cast("String"), schema).alias("loans"))


#Print Schema
loansStreamingJsonDF.printSchema()

loansFlattenedDF = loansStreamingJsonDF.select("loans.time","loans.customer","loans.loanId","loans.status")

#Print Schema
loansFlattenedDF.printSchema()

#Group the data by window and status and compute the count of each group
loanStatusCountsWindowedDF = loansFlattenedDF\
    .groupBy( \
    window(loansFlattenedDF.time, "60 seconds", "30 seconds"),
    loansFlattenedDF.status
).count().orderBy('window')

*****
loanStatusCountsWindowedDF = loansFlattenedDF\
    .groupBy( \
   loansFlattenedDF.status
).count().orderBy('window')   *****


#Print Schema
loanStatusCountsWindowedDF.printSchema()

outputDF =  loanStatusCountsWindowedDF\
                .selectExpr("CAST(status AS STRING) AS key", "to_json(struct(*)) AS value")

# writing to a kafka topic

query = outputDF\
    .writeStream \
    .outputMode("complete") \
    .format("kafka") \
    .option("kafka.bootstrap.servers","localhost:9092") \
    .option("topic","loans-processed") \
    .option("checkpointLocation", "loans") \
    .start()

bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic loans-processed

--------------------------------------------
query = outputDF\
    .writeStream \
    .outputMode("complete") \
    .format("console") \
    .option("checkpointLocation", "loans") \
    .start()

-----------------------------


streaming_df \
    .groupBy("genres") \
    .agg(count("movieId").alias("movie_count"))


finding count of movies ,group by genres, sorting the order by count in last 5 mins




