
Stream processing is a data management technique that involves ingesting a continuous data stream to quickly analyze, 
filter, transform or enhance the data in real time.

tools: pyspark streaming, Apache Flink ,Apache STORM,Kafka

Dstreams -streaming data is convereted into micro batches and each batch processed and generates output



lambda Architecture: realtime + batch processing
    -suported by spark,flink,Apache Druid
    -not supported by storm

Kafka - when incoming data is of huge velocity, and the processing of that data is happening at slower pace ,then we 
        will use kafka as queue to provide some buffer ...

kinesis - aws service to carry the stream data

flume -

------------------------
-------------------------


            from __future__ import print_function

            import sys

            from pyspark import SparkContext
            from pyspark.streaming import StreamingContext

            sc = SparkContext(appName="PythonStreamingNetworkWordCount")
                ssc = StreamingContext(sc, 10)

            lines=ssc.socketTextStream("localhost",9999)              # socketTextStream can be replaced with any source that streams the data

            counts = lines.flatMap(lambda line: line.split(" "))\
                            .map(lambda word: (word, 1))\
                            .reduceByKey(lambda a, b: a+b)

            counts.pprint()

            ssc.start()                                                #actual processing start from here

            ssc.stop()                                                  #to stop processing

            ssc.awaitTermination()


------------------------

batch interval
window duration
sliding interval

            r"""
            Counts words in UTF8 encoded, '\n' delimited text received from the network every second.
            Usage: network_wordcount.py <hostname> <port>
            <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive data.
            To run this on your local machine, you need to first run a Netcat server
                `$ nc -lk 9999`
            and then run the example
                `$ bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999`
            """
            from __future__ import print_function

            import sys

            from pyspark import SparkContext
            from pyspark.streaming import StreamingContext

            if __name__ == "__main__":
                if len(sys.argv) != 4:
                    print("Usage: network_wordcount.py <hostname> <port> <checkpoint-dir>", file=sys.stderr)
                    sys.exit(-1)
                    
                sc = SparkContext(appName="PythonStreamingNetworkWordWindowedCount")
                
                ssc = StreamingContext(sc, 10)

                # Split each line into words
                lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))

                # Checkpoint directory
                ssc.checkpoint(sys.argv[3])

                # Count each word in each batch
                words = lines.flatMap(lambda line: line.split(" "))
                
                pairs = words.map(lambda word: (word, 1))
                
                #Windowing Concept: Reduce last 30 seconds of data, every 10 seconds
                windowedWordCounts = pairs.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 30, 10)
                
                windowedWordCounts.pprint()

                ssc.start()
                ssc.awaitTermination()



---------------------------------------

### Spark Streaming Example with Structured Streaming
#Step 1: Open cloud labs terminal and start netcat utility
        nc -lk 9999

#Step 2: Start PySpark and Run below Spark Streaming program

# Create DataFrame representing the stream of input lines from connection to localhost:9999
lines = spark\
    .readStream\
    .format('socket')\
    .option('host', "localhost")\
    .option('port', 9999)\
    .load()

# Split the lines into words
from pyspark.sql.functions import explode
from pyspark.sql.functions import split

words = lines.select(
    # explode turns each item in an array into a separate row
    explode(
        split(lines.value, ' ')
    ).alias('word')
)

# Generate running word count
wordCounts = words.groupBy('word').count()

# Start running the query that prints the running counts to the console
query = wordCounts\
    .writeStream\
    .outputMode('complete')\
    .format('console')\
    .start()

query.awaitTermination()


---------------------

 --lines.isStreaming

---Windowing
    -sliding window 
        ex: last 15 min interval
    -tumbling window
        ex: b/w 10.00 to 10.30 there are 2 windows with 15 min interval
 
--The “Output” is defined as what gets written out to the external storage. The output can be defined in a different mode:

    Complete Mode - The entire updated Result Table will be written to the external storage. It is up to the storage connector to decide how to handle writing of the entire table.

    Append Mode - Only the new rows appended in the Result Table since the last trigger will be written to the external storage. 
                    This is applicable only on the queries where existing rows in the Result Table are not expected to change.

    Update Mode - Only the rows that were updated in the Result Table since the last trigger will be written to 
                    the external storage (available since Spark 2.1.1). Note that this is different from the Complete Mode in 
                    that this mode only outputs the rows that have changed since the last trigger. 
                    If the query doesn’t contain aggregations, it will be equivalent to Append mode.

---------------------------

lines = spark\
    .readStream\
    .format('socket')\
    .option('host', "localhost")\
    .option('port', 9999)\
    .option('includeTimestamp', 'true')\
    .load()

from pyspark.sql.functions import *

# Split the lines into words, retaining timestamps
# split() splits each line into an array, and explode() turns the array into multiple rows
words = lines.select(
    explode(split(lines.value, ' ')).alias('word'),
    lines.timestamp
)

# Group the data by window and word and compute the count of each group
windowedCounts = words.groupBy(
    window(words.timestamp, "30 seconds", "10 seconds"),
    words.word
).count().orderBy('window')

# Start running the query that prints the windowed word counts to the console
query = windowedCounts\
    .writeStream\
    .outputMode('complete')\
    .format('console') \
    .option("checkpointLocation", "checkpoint1") \
    .option('truncate', 'false')\
    .start()

------------------------------------------

---handling the late arrived events

lines = spark\
    .readStream\
    .format('socket')\
    .option('host', "localhost")\
    .option('port', 9999)\
    .option('includeTimestamp', 'true')\
    .load()

words = lines.select(
    explode(split(lines.value, ' ')).alias('word'),
    lines.timestamp
)

 # Group the data by window and word and compute the count of each group
windowedCounts = words.withWatermark("timestamp", "30 seconds") \
    .groupBy(\
    window(words.timestamp, "30 seconds", "10 seconds"),
    words.word
).count().orderBy('window')


query = windowedCounts\
    .writeStream\
    .outputMode('complete')\
    .format('console') \
    .option("checkpointLocation", "checkpoint") \
    .option('truncate', 'false')\
    .start()

--------------------------------

from pyspark.sql import SparkSession
from pyspark.sql.types import TimestampType, StringType, StructField, StructType
from pyspark.sql.functions import window

# Path to our loan JSON files
inputPath = "/home/ram/futurense_hadoop-pyspark/labs/dataset/loan"

spark = SparkSession \
    .builder \
    .appName("LoanStreamingAnalysis") \
    .getOrCreate()

# Explicitly set schema
schema = StructType([ StructField("time", TimestampType(), True),
                      StructField("customer", StringType(), True),
                      StructField("loanId", StringType(), True),
                      StructField("status", StringType(), True)])

# Create streaming data source
loansStreamingDF = (
  spark
    .readStream
    .schema(schema)
    .option("maxFilesPerTrigger", 1)
    .json(inputPath)
)



# Group the data by window and status and compute the count of each group
loanStatusCountsWindowedDF = loansStreamingDF.withWatermark("time", "30 seconds") \
    .groupBy( \
    window(loansStreamingDF.time, "10 seconds", "5 seconds"),
    loansStreamingDF.status
).count().orderBy('window')



# Stream output to console
query = (
  loanStatusCountsWindowedDF
    .writeStream
    .format("console")
    .outputMode("complete")
    .start()
)


query.awaitTermination()

